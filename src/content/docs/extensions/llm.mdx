---
title: "LLM extension"
---

import { Tabs, TabItem } from '@astrojs/starlight/components';

The goal of the **LLM extension** is to provide a unified interface for using LLMs from popular providers via Kuzu's Cypher interface.
Currently, the extension provides a `CREATE_EMBEDDING` function for calling text embedding model APIs from supported providers.
In the future, we plan on extending this extension to support other functions, such as prompting LLMs.

The `CREATE_EMBEDDING` function supports the following providers:

* [Amazon Bedrock](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-embedding-models.html)
* [Google Vertex](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings)
* [Google Gemini](https://ai.google.dev/gemini-api/docs/embeddings)
* [Ollama](https://ollama.com/blog/embedding-models)
* [OpenAI](https://platform.openai.com/docs/guides/embeddings)
* [Voyage AI](https://docs.voyageai.com/docs/embeddings)

## Usage

```sql
INSTALL llm;
LOAD llm;
```

### Configuration

To allow API calls to the providers, you must set the appropriate environment variables:

| Provider | Required Environment Variables |
| --- | --- |
| `amazon-bedrock` | `AWS_ACCESS_KEY`, `AWS_SECRET_ACCESS_KEY` |
| `google-vertex` | `GOOGLE_CLOUD_PROJECT_ID`, `GOOGLE_VERTEX_ACCESS_KEY` |
| `google-gemini` | `GOOGLE_GEMINI_API_KEY` |
| `open-ai` | `OPENAI_API_KEY` |
| `voyage-ai` | `VOYAGE_API_KEY` |
| `ollama` | None |

See how to set environment variables below:

<Tabs>
<TabItem value="bash" label="Terminal">

```bash
# Set the OpenAI API key
export OPENAI_API_KEY=sk-proj-key
```

</TabItem>

<TabItem value="python" label="Python">
```python
os.environ["OPENAI_API_KEY"] = "sk-proj-key"
#...
```

</TabItem>
</Tabs>

## Creating embeddings

You can generate text embeddings using the `CREATE_EMBEDDING` function. The function returns a `LIST[FLOAT]` type.
The function may be called in the following ways:

```sql
RETURN CREATE_EMBEDDING(
    <PROMPT>,
    <PROVIDER>,
    <MODEL>
);

RETURN CREATE_EMBEDDING(
    <PROMPT>,
    <PROVIDER>,
    <MODEL>,
    <DIMENSIONS>
);

RETURN CREATE_EMBEDDING(
    <PROMPT>,
    <PROVIDER>,
    <MODEL>,
    <REGION/ENDPOINT>
);

RETURN CREATE_EMBEDDING(
    <PROMPT>,
    <PROVIDER>,
    <MODEL>,
    <DIMENSIONS>,
    <REGION/ENDPOINT>
);
```

Required arguments:

- `prompt`: The input text to embed.
    - Type: `STRING`
- `provider`: The embedding provider.
    - Type: `STRING`
    - Example: `open-ai`
- `model`: The identifier of the embedding model.
    - Type: `STRING`
    - Example: `text-embedding-3-small`

Optional arguments:

- `dimensions`: The size of the embedding vector.
    - Type: `INT64`
    - Supported providers: `open-ai`, `google-vertex`, `voyage-ai`
    - Example: `256` for `google-vertex`
    - Note: This value must be positive.
- `region`: The region to send requests to.
    - Type: `STRING`
    - Supported providers: `amazon-bedrock`, `google-vertex`
    - Example: `us-east-1` for `amazon-bedrock`
- `endpoint`: The endpoint to send requests to.
    - Type: `STRING`
    - Supported providers: `ollama`
    - Default: `http://localhost:11434`

:::note[Note]
If an unsupported model, dimension, region, or endpoint is specified, the API call to the provider will fail. Check the provider's API documentation for the currently supported options.
:::

### Examples

```sql
RETURN CREATE_EMBEDDING("Hello world", "amazon-bedrock", "amazon.titan-embed-text-v1", "us-east-1");

RETURN CREATE_EMBEDDING("Hello world", "google-gemini", "gemini-embedding-exp-03-07");
RETURN CREATE_EMBEDDING("Hello world", "google-vertex", "gemini-embedding-001", 256, "us-east1");

RETURN CREATE_EMBEDDING("Hello world", "ollama", "nomic-embed-text");
RETURN CREATE_EMBEDDING("Hello world", "ollama", "nomic-embed-text", "http://endpoint.example.com:8000");

RETURN CREATE_EMBEDDING("Hello world", "open-ai", "text-embedding-3-small");

RETURN CREATE_EMBEDDING("Hello world", "voyage-ai", "voyage-3-large", 512);
```

## Storing embeddings in a vector index

The `CREATE_EMBEDDING` function can be used together with the `vector` extension to store and search embeddings
directly in Kuzu. Refer to the [vector search docs](/extensions/vector) for an example.
