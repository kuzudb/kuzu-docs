---
title: "LLM Extension"
---

The **LLM extension** provides an interface to fetch text embeddings from supported providers and their models.

It can be installed and loaded using the following commands:

```sql
INSTALL llm;
LOAD llm;
```

See the list of providers and their embedding documentation.

* [OpenAI](https://platform.openai.com/docs/guides/embeddings)
* [Amazon Bedrock](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-embedding-models.html)
* [Google Vertex](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings)
* [Google Gemini](https://ai.google.dev/gemini-api/docs/embeddings)
* [Voyage AI](https://docs.voyageai.com/docs/embeddings)
* [Ollama](https://ollama.com/blog/embedding-models)

:::note[Note]
If an unsupported region or model is specified, the API call to the provider will result in an error.
:::
---

## Usage

### Configuration

The LLM extension supports configurable options such as credentials, dimensions, and region where applicable.

- All credentials are provided through environment variables.
- Dimensions and region are passed to the `CREATE_EMBEDDING` function call, when supported.

The following environment variables must be set for the corresponding providers:

| Provider | Required Environment Variables |
| --- | --- |
| `amazon-bedrock` | `AWS_ACCESS_KEY`, `AWS_SECRET_ACCESS_KEY` |
| `google-vertex` | `GOOGLE_CLOUD_PROJECT_ID`, `GOOGLE_VERTEX_ACCESS_KEY` |
| `google-gemini` | `GOOGLE_GEMINI_API_KEY` |
| `open-ai` | `OPENAI_API_KEY` |
| `voyage-ai` | `VOYAGE_API_KEY` |

:::note[Note]
The `ollama` provider does not need any configured environment variables but expects the embedding model to be available at `http://localhost:11434`.
:::
---

## Embedding Function

The core feature of the LLM extension is a scalar function that generates text embeddings.

### Syntax

```sql
CREATE_EMBEDDING(prompt: STRING, provider: STRING, model: STRING) -> LIST<FLOAT>
```

### Overloads

```sql
CREATE_EMBEDDING(prompt: STRING, provider: STRING, model: STRING, dimensions: INT64) -> LIST<FLOAT>
```

```sql
CREATE_EMBEDDING(prompt: STRING, provider: STRING, model: STRING, region: STRING) -> LIST<FLOAT>
```

```sql
CREATE_EMBEDDING(prompt: STRING, provider: STRING, model: STRING, dimensions: INT64, region: STRING) -> LIST<FLOAT>
```

### Parameters

- **prompt**: The input text to embed.
- **provider**: The embedding provider, e.g., `'open-ai'`, `'ollama'`, `'voyage-ai'`, etc.
- **model**: The identifier of the embedding model. This may change overtime; reference the provider's API reference before use.
- **dimensions** *(optional)*: The size of the embedding vector to be returned, if supported by the provider. Must be a positive integer.
- **region** *(optional)*: A region or endpoint hint, if supported by the provider.

### Examples

```sql
CREATE_EMBEDDING("Hello world", "openai", "text-embedding-3-small")
```

```sql
CREATE_EMBEDDING("Hello world", "ollama", "nomic-embed-text")
```

```sql
CREATE_EMBEDDING("Hello world", "google-gemini", "gemini-embedding-exp-03-07")
```

With optional parameters:

```sql
CREATE_EMBEDDING("Hello world", "voyage-ai", "voyage-3-large", 512)
```

```sql
CREATE_EMBEDDING("Hello world", "amazon-bedrock", "amazon.titan-embed-text-v1", "us-east-1");
```

```sql
CREATE_EMBEDDING("Hello world", "google-vertex", "gemini-embedding-001", 256, "us-east1");
```

---

### Dimensions Support

The following providers support configuring the number of dimensions of the embedding.

* open-ai
* google-vertex
* voyage-ai

:::note[Note]
The list of models that support dimension configuration may change
overtime. The set of valid dimensions is determined by the provider and may also
change over time. It is highly recommended to check the provider's API reference
before use. If an unsupported dimension is specified, the provider's API will return an error.
:::
---

## Synergy with Vector Extension
The embedding function can be used with the vector extension. Here, we show how to create embeddings within Kuzu that can be inserted into a vector index, by modifying this example described in the vector docs.

```python
# create_embeddings.py
import pyarrow
import polars
import kuzu

DB_NAME = ""

# Initialize the database
db = kuzu.Database(DB_NAME)
conn = kuzu.Connection(db)

# Install and load vector extension
conn.execute("INSTALL vector; LOAD vector;")
conn.execute("INSTALL llm; LOAD llm;")


# Create tables
conn.execute("CREATE NODE TABLE Book(id SERIAL PRIMARY KEY, title STRING, title_embedding FLOAT[1536], published_year INT64);")
conn.execute("CREATE NODE TABLE Publisher(name STRING PRIMARY KEY);")
conn.execute("CREATE REL TABLE PublishedBy(FROM Book TO Publisher);")

# Sample data
titles = [
    "The Quantum World",
    "Chronicles of the Universe",
    "Learning Machines",
    "Echoes of the Past",
    "The Dragon's Call"
]
publishers = ["Harvard University Press", "Independent Publisher", "Pearson", "McGraw-Hill Ryerson", "O'Reilly"]
published_years = [2004, 2022, 2019, 2010, 2015]

# Insert sample data - Books with embeddings
for title, published_year in zip(titles, published_years):
    conn.execute(
        """CREATE (b:Book {title: $title, title_embedding: create_embedding($title, 'open-ai', 'text-embedding-3-small'), published_year: $year});""",
        {"title": title, "year": published_year}
    )
    print(f"Inserted book: {title}")

# Insert sample data - Publishers
for publisher in publishers:
    conn.execute(
        """CREATE (p:Publisher {name: $publisher});""",
        {"publisher": publisher}
    )
    print(f"Inserted publisher: {publisher}")

# Create relationships between Books and Publishers
for title, publisher in zip(titles, publishers):
    conn.execute("""
        MATCH (b:Book {title: $title})
        MATCH (p:Publisher {name: $publisher})
        CREATE (b)-[:PublishedBy]->(p);
        """,
            {"title": title, "publisher": publisher}
    )
    print(f"Created relationship between {title} and {publisher}")



conn.execute(
            """
            CALL CREATE_VECTOR_INDEX(
                'Book',
                'title_vec_index',
                'title_embedding'
                )
            """
            )

result = conn.execute(
    """
    CALL QUERY_VECTOR_INDEX(
        'Book',
        'title_vec_index',
        create_embedding('quantum machine learning', 'open-ai', 'text-embedding-3-small'),
        2
    )
    RETURN node.title ORDER BY distance;
    """
)
print(result.get_as_pl())
```

The result shows the 2 book titles most similar to the query "quantum machine learning".

```
┌───────────────────┐
│ node.title        │
│ ---               │
│ str               │
╞═══════════════════╡
│ Learning Machines │
│ The Quantum World │
└───────────────────┘
```
