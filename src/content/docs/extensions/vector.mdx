---
title: "Vector search"
---

import { Tabs, TabItem } from '@astrojs/starlight/components';

The `VECTOR` extension provides a **native**, **disk-based** [HNSW](https://en.wikipedia.org/wiki/Hierarchical_navigable_small_world) vector index
for accelerating similarity search over your vector embeddings (32-bit and 64-bit float arrays) that are stored as node properties in Kuzu.

The vector extension provides the following functions:
| Function | Description |
| --- | --- |
| `CREATE_VECTOR_INDEX` | Create the index |
| `QUERY_VECTOR_INDEX` | Query the index |
| `DROP_VECTOR_INDEX` | Drop the index |


## Usage

Install and load the vector extension as follows:

```sql
INSTALL VECTOR;
LOAD VECTOR;
```

### Preparing the data

To create a vector index, you need a node table with a property of type `LIST[FLOAT]` storing text embeddings.
Below is an example demonstrating two ways in which such a dataset can be created. The first is to use some external
library, such as `sentence_transformers` in Python to create the embeddings. Alternatively, you can use Kuzu's [`llm`](/extensions/llm)
extension to directly create the embeddings using Cypher.

```python
# create_embeddings.py
# pip install sentence-transformers
import kuzu

use_llm_extension = True # Set to `False` to use sentence_transformers

DB_NAME = "ex_kuzu_db"

# Initialize the database
db = kuzu.Database(DB_NAME)
conn = kuzu.Connection(db)

if use_llm_extension:
    conn.execute("INSTALL llm; LOAD llm;")
else:
    # Load a pre-trained embedding generation model
    # https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
    from sentence_transformers import SentenceTransformer
    model = SentenceTransformer("all-MiniLM-L6-v2")

# Install and load vector extension
conn.execute("INSTALL vector; LOAD vector;")

# Create tables
conn.execute("CREATE NODE TABLE Book(id SERIAL PRIMARY KEY, title STRING, title_embedding FLOAT[384], published_year INT64);")
conn.execute("CREATE NODE TABLE Publisher(name STRING PRIMARY KEY);")
conn.execute("CREATE REL TABLE PublishedBy(FROM Book TO Publisher);")

# Sample data
titles = [
    "The Quantum World",
    "Chronicles of the Universe",
    "Learning Machines",
    "Echoes of the Past",
    "The Dragon's Call"
]
publishers = ["Harvard University Press", "Independent Publisher", "Pearson", "McGraw-Hill Ryerson", "O'Reilly"]
published_years = [2004, 2022, 2019, 2010, 2015]

# Insert sample data - Books with embeddings
for title, published_year in zip(titles, published_years):
    if use_llm_extension:
        os.environ["OPENAI_API_KEY"] = "sk-proj-key" # Replace with your own OpenAI API key
        conn.execute(
            """CREATE (b:Book {title: $title, title_embedding: create_embedding($title, 'open-ai', 'text-embedding-3-small', 384), published_year: $year});""",
            {"title": title, "year": published_year}
        )
    else:
        embeddings = model.encode(title).tolist()
        conn.execute(
            """CREATE (b:Book {title: $title, title_embedding: $embeddings, published_year: $year});""",
            {"title": title, "embeddings": embeddings, "year": published_year}
        )

    print(f"Inserted book: {title}")

# Insert sample data - Publishers
for publisher in publishers:
    conn.execute(
        """CREATE (p:Publisher {name: $publisher});""",
        {"publisher": publisher}
    )
    print(f"Inserted publisher: {publisher}")

# Create relationships between Books and Publishers
for title, publisher in zip(titles, publishers):
    conn.execute("""
        MATCH (b:Book {title: $title})
        MATCH (p:Publisher {name: $publisher})
        CREATE (b)-[:PublishedBy]->(p);
    """,
    {"title": title, "publisher": publisher}
    )
    print(f"Created relationship between {title} and {publisher}")
```

The embeddings are generated on the title properties of each `Book` and
ingested into the Kuzu database.

### Creating a vector index

Create a vector index as follows:
```cypher
CALL CREATE_VECTOR_INDEX(
    'table_name',      // Name of the table containing the vector column
    'index_name',      // Name to identify the vector index
    'column_name',     // Name of the column containing vector embeddings
    [option_name := option_value]  // Optional parameters for index configuration
);
```
:::note[Immutable index]
Once created, the vector index cannot be modified. If the data in the underlying table changes,
you must drop and re-create the index.
:::

We support the following options during index creation.
The HNSW index in Kuzu is structured with two hierarchical layers.
The lower layer includes all vectors, while the upper layer contains a sampled subset of the lower layer.
| option name | description | default |
| :---------: | ------------ | ---------- |
| `mu`                | Max degree of nodes in the upper graph. It should be smaller than ml. A higher value leads to a more accurate index, but increase the index size and construction time. | 30 |
| `ml`                 | Max degree of nodes in the lower graph. It should be larger than mu. A higher value leads to a more accurate index, but increase the index size and construction time. | 60 |
| `pu`                | Percentage of nodes sampled into the upper graph in the range of `[0.0, 1.0]`. | 0.05 |
| `metric`       | Metric (distance computation) functions. Supported values are `cosine`, `l2`, `l2sq`, and `dotproduct`. | cosine |
| `efc`               | The number of candidate vertices to consider during the construction of the index. A higher value will result in a more accurate index, but will also increase the time it takes to build the index. | 200 |

In our example, we create a vector index over the `title_embedding` column from `Book` table.

<Tabs>
<TabItem value="cypher" label="Cypher">
```cypher
CALL CREATE_VECTOR_INDEX(
    'Book',
    'title_vec_index',
    'title_embedding',
    metric := 'l2'
);
```
</TabItem>
<TabItem value="python" label="Python">
```python
# Define connection to the database
# ...
conn.execute(
    """
    CALL CREATE_VECTOR_INDEX(
        'Book',
        'title_vec_index',
        'title_embedding',
        metric := 'l2'
    );
    """
)
print("Vector index created")
```
</TabItem>
</Tabs>

### Querying the vector index
To perform similarity search using the vector index, use the `QUERY_VECTOR_INDEX` function:

```cypher
CALL QUERY_VECTOR_INDEX(
    'table_name',      // Name of the table
    'index_name',      // Name of the vector index
    query_vector,      // Vector to search for
    k,                 // Number of nearest neighbors to return
    [option_name := option_value]  // Optional parameters
) RETURN node.id ORDER BY distance;
```

We return nodes, which can be referenced by `node` and their distance from the query vector, which can be referenced by `distance`.
You can use `YIELD` to rename the result columns. More details on `YIELD` can be found [here](/cypher/query-clauses/call/#yield).
By default, the returned result from `QUERY_VECTOR_INDEX` is not sorted.
To get sorted result on distance, you need to manually specify `ORDER BY distance` in the `RETURN` clause.

#### Search Options
The following options can be used to tune the search behavior:

| Option | Description | Default |
|--------|-------------| ------- |
| efs    | Number of candidate vertices to consider during search. Higher values increase accuracy but also increase search time. | 200 |

#### Example search queries

Let's run some example search queries on our newly created vector index. We can again either use the `llm` extension to create the embeddings
or use a third party library.

```python
import kuzu

use_llm_extension = True # Set to `False` to use sentence_transformers

DB_NAME = "ex_kuzu_db"

# Initialize the database
db = kuzu.Database(DB_NAME)
conn = kuzu.Connection(db)

# Install and load vector extension once again
conn.execute("INSTALL VECTOR;")
conn.execute("LOAD VECTOR;")

if use_llm_extension:
    conn.execute("INSTALL llm; LOAD llm;")
else:
    from sentence_transformers import SentenceTransformer
    # Load a pre-trained embedding generation model
    # https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
    model = SentenceTransformer("all-MiniLM-L6-v2")

if use_llm_extension:
    os.environ["OPENAI_API_KEY"] = "sk-proj-key" # Replace with your own OpenAI API key
    result = conn.execute(
        """
        CALL QUERY_VECTOR_INDEX(
            'Book',
            'title_vec_index',
            create_embedding('quantum machine learning', 'open-ai', 'text-embedding-3-small', 384),
            2
        )
        RETURN node.title ORDER BY distance;
        """)
else:
    query_vector = model.encode("quantum machine learning").tolist()
    result = conn.execute(
        """
        CALL QUERY_VECTOR_INDEX(
            'Book',
            'title_vec_index',
            $query_vector,
            2,
            efs := 500
        )
        RETURN node.title ORDER BY distance;
        """,
        {"query_vector": query_vector})

print(result.get_as_pl())
```
In the above query, we asked for the 2 nearest neighbors of the query vector "quantum machine learning".
The result is a list of book titles that are most similar to this concept.

```
┌───────────────────┐
│ node.title        │
│ ---               │
│ str               │
╞═══════════════════╡
│ The Quantum World │
│ Learning Machines │
└───────────────────┘
```

Next, let's use the vector index to find an entry point to the graph, following which we do a graph
traversal to find the names of publishers of the books.

```python
result = conn.execute(
    """
    CALL QUERY_VECTOR_INDEX('book', 'title_vec_index', $query_vector, 2)
    WITH node AS n, distance
    MATCH (n)-[:PublishedBy]->(p:Publisher)
    RETURN p.name AS publisher, n.title AS book, distance
    ORDER BY distance LIMIT 5;
    """,
    {"query_vector": query_vector})
print(result.get_as_pl())
```
In the above query, we once asked for the 2 nearest neighbors of the query vector "quantum machine learning".
But this time, we use the `node` and `distance` variables to return the book publishers, the book titles, and the distance
between the query vector and the book title vector. The results are sorted by distance.
```
┌──────────────────────────┬───────────────────┬──────────┐
│ publisher                ┆ book              ┆ distance │
│ ---                      ┆ ---               ┆ ---      │
│ str                      ┆ str               ┆ f64      │
╞══════════════════════════╪═══════════════════╪══════════╡
│ Harvard University Press ┆ The Quantum World ┆ 0.311872 │
│ Pearson                  ┆ Learning Machines ┆ 0.415366 │
└──────────────────────────┴───────────────────┴──────────┘
```

Using vector search in combination with graph traversal in this manner can be a powerful technique to
find semantically related entities in a graph.

### Index Management

#### Drop an Index
To remove a vector index, use the `DROP_VECTOR_INDEX` function:

```cypher
CALL DROP_VECTOR_INDEX('Book', 'title_vec_index');
```

#### List All Indexes
View all created indexes in the database using `SHOW_INDEXES`:

```cypher
CALL SHOW_INDEXES() RETURN *;
```

Example output:
```
┌────────────┬─────────────────┬────────────┬───────────────────┬──────────────────┬───────────────────────────────┐
│ table name │ index name      │ index type │ property names    │ extension loaded │ index definition              │
├────────────┼─────────────────┼────────────┼───────────────────┼──────────────────┼─────────────────────────────--┤
│ Book       │ title_vec_index │ HNSW       │ [title_embedding] │ True             │ CALL CREATE_VECTOR_INDEX(...) │
└────────────┴─────────────────┴────────────┴───────────────────┴──────────────────┴───────────────────────────────┘
```

## Filtered vector search
Kuzu allows you to combine vector search with filter predicates, by using [projected graphs](/extensions/algo#project-graph).

For example, we can search for books similar to "quantum world", but only those that were published after 2010.

<Tabs>
<TabItem value="python" label="Python">
```python
# Pass in an existing connection
# ...

# Step 1: Create a projected graph that filters books by publication year
conn.execute(
    """
    CALL PROJECT_GRAPH(
        'filtered_book',
        {'Book': 'n.published_year > 2010'},
        []
    );
    """
)

# Step 2: Perform vector similarity search on the filtered subset

if use_llm_extension:
    os.environ["OPENAI_API_KEY"] = "sk-proj-key" # Replace with your own OpenAI API key

    result = conn.execute("""
    CALL QUERY_VECTOR_INDEX(
        'filtered_book',
        'title_vec_index',
        create_embedding("quantum_world", "open-ai", "text-embedding-3-small", 384),
        2
    )
    WITH node AS n, distance as dist
    MATCH (n)-[:PublishedBy]->(p:Publisher)
    RETURN n.title AS book,
           n.published_year AS year,
           p.name AS publisher
    ORDER BY dist;
    """)
else:
    query_vector = model.encode("quantum world").tolist()
    result = conn.execute("""
        CALL QUERY_VECTOR_INDEX(
            'filtered_book',
            'title_vec_index',
            $query_vector,
            2
        )
        WITH node AS n, distance as dist
        MATCH (n)-[:PublishedBy]->(p:Publisher)
        RETURN n.title AS book,
               n.published_year AS year,
               p.name AS publisher
        ORDER BY dist;
        """,
        {"query_vector": query_vector})
print(result.get_as_pl())
```
</TabItem>
<TabItem value="cypher" label="Cypher">

```bash
# Set the OpenAI API key
export OPENAI_API_KEY=sk-proj-key
```

```cypher
INSTALL llm;
LOAD llm;

// Step 1: Create a projected graph that filters books by publication year
CALL PROJECT_GRAPH(
    'filtered_book',   // Name of the projected graph
    {'Book': 'n.published_year > 2010'},   // Projected node table Book with a filter on published_year. `n` is a place_holder here to reference the node table.
    []   // No relationship tables can be projected.
);

// Step 2: Perform vector similarity search on the filtered subset
In the `QUERY_VECTOR_INDEX` function, we can pass in the name of the projected graph as `table_name` parameter.
CALL QUERY_VECTOR_INDEX(
    'filtered_book', // Name of the projected graph
    'title_vec_index', // Name of the index
    create_embedding('quantum world', 'open-ai', 'text-embedding-3-small', 384),
    2
)
WITH node AS n, distance as dist
MATCH (n)-[:PublishedBy]->(p:Publisher)
RETURN n.title AS book,
        n.published_year AS year,
        p.name AS publisher
ORDER BY dist;
```
</TabItem>
</Tabs>

The result shows the two most similar books to the query "quantum world". Although we have a book named
"The Quantum World" in the original dataset, it does not appear in the results because it was published before 2010.

```
shape: (2, 3)
┌────────────────────────────┬──────┬───────────────────────┐
│ book                       ┆ year ┆ publisher             │
│ ---                        ┆ ---  ┆ ---                   │
│ str                        ┆ i64  ┆ str                   │
╞════════════════════════════╪══════╪═══════════════════════╡
│ Chronicles of the Universe ┆ 2022 ┆ Independent Publisher │
│ Learning Machines          ┆ 2019 ┆ Pearson               │
└────────────────────────────┴──────┴───────────────────────┘
```
