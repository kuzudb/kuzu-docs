---
slug: kuzu-0.0.9-release
authors: [team]
tags: [release]
---

# Kùzu 0.0.9 Release

We are very happy to release Kùzu 0.0.9 today! This release comes with the following new main features and improvements: 

## New Features

### Load From
Kùzu now supports direct query over a file without loading into database through `LOAD FROM` clause. E.g. the following query counts number of rows whose first column starts with 'Adam'.
```
LOAD FROM "user.csv" 
WHERE column0 =~ 'Adam*'
RETURN COUNT(*)
```
`LOAD FROM` can also be used as the input source of bulk update.
```
LOAD FROM "user.csv"
CREATE (:Person {name: column0, age: to_int64(column1)});
```

#### Header Schema
By default, Kùzu will read the header of the file to detect column name and type. If no header is avaliable it will use auto-generated name and parse as default string type. To manually specify the header, you can use `LOAD WITH HEADERS ... FROM ...`. 

E.g. the following query will bind first column to `name` with STRING type and second column to `age` with INT64 type.
```
LOAD WITH HEADERS (name STRING, age INT64) FROM "user.csv" 
WHERE name =~ 'Adam*'
RETURN name, age;
```

If header is manually specified, Kùzu will try to cast to the given type and throw exceptions if cast fails. More information can be found at [Load From](../docs/cypher/query-clauses/load_from.md).

### Transaction Statement
This release replaces the `beginReadTransaction()`, `beginWriteTransaction()`, `commit()` and `rollback()` APIs in all language bindings with statements. E.g.
```
BEGIN TRANSACTION;
CREATE (a:User {name: 'Alice', age: 72});
MATCH (a:User) RETURN *;
COMMIT;
``` 
The above sequence of statements starts a manual write transaction, adds a new node, and within the same transaction also reads all of the tuples in User table. Finally, commits the transaction. More transaction statement can be found at [Transaction](../docs/cypher/transaction.md).

### Comment on Table
You can now add comments to a table using `COMMENT ON TABLE` statement. The following query adds a comment to `User` table.
```
COMMENT ON TABLE User IS 'User information';
```
Comments can be extracted through `SHOW_TABLES()` function.
```
CALL SHOW_TABLES() RETURN *;
--------------------------------------------
| TableName | TableType | TableComment     |
--------------------------------------------
| User      | NODE      | User information |
--------------------------------------------
| City      | NODE      |                  |
--------------------------------------------
```

### Recursive Rel Projection
This release expand recursive rel pattern and enables projection on intermediate nodes and rels. Previously, Kùzu only supports returning all properties on
```
MATCH (a:User)-[e:Follows*1..2]->(b:User) 
RETURN nodes(e), rels(e);
```
This approach incurs significant computation overhead when user only interested in a subset of properties over path. Also returning all properties make the result hard to interpret.

Kùzu now allow projection inside recursive rel pattern using a list-comprehension-like syntax.
```
MATCH (a:User)-[e:Follows*1..2 (r, n | WHERE r.since > 2020 | {r.since}, {n.name})]->(b:User) 
RETURN nodes(e), rels(e);
```
The query above finds all 1 to 2 hops Follows path since 2020. The project `since` property of intermediate rels and `name` property of intermediate nodes along the path.

For more information, check [Project Intermediate Nodes and Rels](../docs/cypher/query-clauses/match.md#project-intermediate-nodes-and-rels).

The performance improvement is shown at the [Performance Improvements](#performance-improvements) section.
<!--> We don't support `{}, {}`. Is this a bug or expected? <-->

### CREATE REL GROUP[^1]

We have recieved many feedbacks regarding the limitation that a relelationship table can only be defined over a pair of from-to node tables. This release introduces `CREATE REL GROUP` statement which has a similar syntax as `CREATE REL TABLE` but allows multiple `FROM ... TO ...`. This statement will create a relationship table for each `FROM ... TO ...` internally. User can query with rel table group as the union of all rel tables in the group.

E.g. the following statement will create a Knows_User_User rel table and a Knows_User_City rel table.
```
CREATE REL TABLE GROUP Knows (FROM User To User, FROM User to City, year INT64);
```
To query with rel group, simply treat it as a rel table. E.g.
```
MATCH (a:User)-[:Knows]->(b) RETURN *;
```
The query above is equivalent to
```
MATCH (a:User)-[:Knows_User_User|:Knows_User_city]->(b) RETURN *;
```
**Note**
- For `COPY FROM` and `CREATE`, we currently don't support using rel group so you need to explicitly specify a rel table.
- You can drop or rename a rel table group in the same way as a rel table.

### Data Types & Functions
We introduces a few more numerical data types
- INT8: 1 byte signed integer
- UINT64: 8 bytes unsigned integer
- UINT32: 4 bytes unsigned integer
- UINT16: 2 bytes unsigned integer
- UINT8: 1 bytes unsigned integer

We have also added a several casting functions and list functions. See [functions](../docs/cypher/expressions/) for more information.

## Performance Improvements

### New CSV and Parquet Reader
In this release, we start to replace arrow's CSV and Parquet reader with more lightweight and customized implementations.

Following DuckDB's implementation, we've replaced arrow's streaming CSV reader with a parallel one. The parallel CSV reader assumes no multi-line strings and provide a performance boost linear to the number of threads given.

If multi-line string presents, CSV reader will fail and you need to fall back to single thread mode by setting `parallel=false` (See [Data Import from CSV Files](../docs/data-import/csv-import.md#data-import-from-csv-files)).

We demonstrate the performance of our parallel csv reader through the new [LOAD FROM](#load-from) feature as follows.
```
LOAD FROM "ldbc-100/comment_0_0.csv" (header = true, delim = '|') RETURN COUNT(*);
```

| # threads |   1   |   2   |   4   |   8   |  16   |
| --------- | ----- | ----- | ----- | ----- | ----- |
| Time (s) | 297.19 | 170.71 (1.7x) | 109.38 (2.7x) | 69.01 (4.3x) | 53.28 (5.6x) |

### Bitpacking Compression
Integers are common data types in database systems.
Being able to compress them can help reduce disk space and I/Os.
In this release, we introduced bitpacking compression.
It is best for small integers, which can be encoded more compactly with less bits.

To show the difference, we take `length` column from ldbc `Comment` table as an example, which is of `INT32` data type and whose values ranges from 2 to 1998.
Together with an auto-increment `ID` column as the primary key, we create a node table `(ID INT64, length INT32, PRIMARY KEY(ID))`. The loaded data file size, and loading time is listed in the below table. Data file size is largely reduced from 2.6GB to 1.1GB (2.4x), while the data loading time stays the same (75.69s vs. 75.84s).

Reduced data file size also helps reduce disk I/Os, which can improve query scan performance.
We show that with a query that sum all length values.
```
MATCH (l:length) RETURN sum(l.length);
```
The query time improves from 1.64s to 0.45s (3.6x).

|                 | Data size | Loading time   | Query time   |
| --------------- | --------- | -------------- | ------------ |
| w/o compression | 2.6GB     | 75.69s     | 1.64s    |
| w/ compression  | **1.1GB (2.4x)** | **75.84s** | **0.45s (3.6x)** |

More compressions on integers, floats, and string values will come soon. Please stay tuned!
<!--> 181733.98ms 736.22ms 867M | 180010.68ms 263.95ms 319M <-->

Note: The compression is now only applied on node tables. It will be adapted to rel tables too in our next release. By default, we turn on compression for all node tables. To disable it, we provide an option when starting the database. For example, starting our CLI with `--nocompress` option can disable compression on all write statements to node tables.

### General Data Loading Improvement
Data loading time is improved due the following changes
- parallel csv reader.
- bitpacking compression to write less data to disk.
- remove line counting when copying rel tables.
- dedicated casting functions to avoid string copy.
- reduced hash index file size.

| Files            | # lines     | CSV file size | v0.0.8      | v0.0.9      |
| ---------------- | ----------- | ------------- | ----------- | ----------- |
| comment.csv      | 220M        | 22.49 GB      | 187.76s     | **131.48s** |
| person.csv       | 0.45M       | 43.6M         | 1.16s       | **0.78s**   |
| likesComment.csv | 242M        | 13 GB         | 250.64s     | **210.72s** |
| knows.csv        | 20M         | 1.1 GB        | 24.40s      | **19.54s**  |

[^1]: This is an experimental feature and might be changed in the future.


### Projection pushdown to recursive joins
```
MATCH (a:Person)-[e:knows*1..2]->(b:Person) 
WHERE a.ID<1000
RETURN properties(rels(e), 'creationDate'), properties(nodes(e), 'ID');
```

```
MATCH (a:Person)-[e:knows*1..2 (r, n | {r.creationDate}, {n.ID})]->(b:Person) 
WHERE a.ID<1000
RETURN properties(rels(e), 'creationDate'), properties(nodes(e), 'ID');
```

|          | w/ projection pushdown | w/o projection pushdown |
| -------- |---------------------- | ----------------------- |
| Cold run | 1.73 | 1.89 |
| Hot run  | 1.53 | 1.71 |
