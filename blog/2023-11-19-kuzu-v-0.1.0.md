---
slug: kuzu-0.1.0-release
authors: [team]
tags: [release]
---


# Kùzu 0.1.0 Release

We are very happy to release Kùzu 0.1.0 today! This is a major release with the following features and improvements:

<!--truncate-->

## NodeGroup-Based Storage

With this release, we have completed the major features of our NodeGroup-base storage design, 
which was outlined in this [issue](https://github.com/kuzudb/kuzu/issues/1474). The primary goal of this design was to have a 
storage that was conducive to implementing compression and zone maps optimization. 
Conceptually NodeGroup is equivalent to [Parquet RowGroup](https://parquet.apache.org/docs/concepts/) which 
represents a horizontal partition of a table consisting of k many nodes (k=64x2048 for now). Each k nodes' data are 
managed and compressed as a unit on disk files. In release v0.0.7, we had completed the first part of this design and changed our
node table storage to use NodeGroups. In this release, we have completed the second part of this design and now relationship 
tables are also stored as NodeGroups. That means we now compress the relationships of k many nodes together.

We also stores all column data in a single file `data.kz` which has significantly reduced the number of database files we now maintain.

### String Compression

We are extending now compress strings in the database using dictionary compression. 
For each string "column chunk" (which is a partition of an entire column in a table 
storing one NodeGroup's values), each string s is
stored once in a dictionary, and for each record that has value s, we store a pointer to s. 
This design applies when storing string properties on relationship tables.
This is done by using 3 column chunks in total. 2 column chunks store the dictionary as follows. One "raw strings" column chunk
stores all the unique strings in the column chunk one after another, and another "offsets column chunk" other one identifies
the beginning indices of each string. Then, one additional "index column chunk" stores the pointers to the strings
as indices to the "offsets" column to identify the strings.
The offset and index columns are bitpacked in the manner of integer columns.

**String Compression Benchmark**

Here is a micro-benchmark using the Comment table in LDBC100. To compare the compression rate of each column individually, 
we construct a new table Tx for each string column x in the Comment table, e.g., `Browser Used`. Tx consists of the 
column x and a serial primary key, which allows us to avoid storing any materialized hash index. We report the size of the data.kz file
and compare against the previous version v.0.0.10 of Kùzu.

| Column | Version 0.0.10 | Version 0.1.0 | Difference |
|---------------|----------------|----------------|------------------|
| Browser Used  | 4.2 GB         | 272 MB         | -93.5%          |
| Content       | 9.7 GB         | 7.5 GB         | -22.7%          |
| Location IP   | 5 GB           | 1.6 GB         | -68.0%          |

We also report the entire LDBC100 database size, including all database files (data.kz, indices, metadata, catalog), of v 0.1.0
and a slightly older version v 0.0.8, which included compression of nodes. So this experiment reports 
both improvements that come from storing relationship tables in compressed form as well as 
storing strings of both node and relationship tables in compressed form.

| Database | Version 0.0.8 | Version 0.1.0 | Difference |
|----------|----------------|--------------|----------------|
| LDBC100  |   127 GB       |     94 GB    |     -26.0%    |


<!--
|         | Version 0.0.10 | Version 0.1.0 |
|---------|----------------|---------------|
| 40 threads | 130.609s      | 131.219s      |
| 20 threads | 130.249s      | 133.606s      |
| 10 threads | 129.738s      | 129.655s      |
| 8 threads  | 130.158s      | 122.814s      |
| 4 threads  | 160.329s      | 171.814s      |
| 2 threads  | 270.545s      | 292.094s      |
| 1 threads  | 512.480s      | 549.882s      |
-->
<!--
|         | Version 0.0.10 | Version 0.1.0 | Performance Difference |
|---------|----------------|----------------|-------------------------|
| 40 threads | 244.193s      | 187.652s      | +23.08%                 |
| 20 threads | 239.545s      | 191.811s      | +24.96%                 |
| 10 threads | 251.105s      | 226.691s      | +10.24%                 |
| 8 threads  | 266.763s      | 229.821s      | +13.81%                 |
| 4 threads  | 312.461s      | 246.847s      | +21.02%                 |
| 2 threads  | 446.717s      | 335.631s      | +24.81%                 |
| 1 threads  | 700.847s      | 581.885s      | +16.96%                 |
-->

### Data Ingestion Improvements
Moving our relationship table storage to a NodeGroup-based one also improved our
data ingestion times. The following benchmark reports the loading time of LDBC100 `likesComment.csv` relationship records. 
The file contains 242M records and takes 13 GB in raw CSV format. Below we compare v 0.1.0 against v 0.0.10 using machine with 
XXX CPU with Y many cores and X GB buffer pool XXX.

|         | Version 0.0.10 | Version 0.1.0 |  Difference |
|---------|----------------|----------------| ----------------|
| 8 threads  | 266.8 s      | 229.8 s      | -13.9%           |
| 4 threads  | 312.5 s      | 246.8 s      | -21.0%
| 2 threads  | 446.7 s      | 335.6 s      | -24.8%
| 1 threads  | 700.8 s      | 581.9 s      | - 17.0%


## New Features

### Direct Scans of DataFrames 
We now support scanning over Pandas DataFrames as regular tables. Consider the following `person` DataFrame
that contains two columns, `id` and `height_in_cm` (only the latter will be used in the example):

```
id = np.array([0, 2, 3, 5, 7, 11, 13], dtype=np.int64)
height_in_cm = np.array([167, 172, 183, 199, 149, 154, 165], dtype=np.uint32)
person = pd.DataFrame({'id': id, 'height': height_in_cm})
```
The query below finds all students who is taller than the average height of the records in the `person` DataFrame:
```
query = 'CALL READ_PANDAS("person")
         WITH avg(height / 2.54) as height_in_inch
         MATCH (s:student)
         WHERE s.height > height_in_inch
         RETURN s'
results = conn.execute(query)
```

Details of this feature can be found [here](../docs/cypher/query-clauses/call.md#read_pandas).

### Copy
This release comes with several new features related to Cypher's `COPY` clause.

#### Copy To Parquet Files
Query results can now be exported to Parquet format.
```
COPY ( MATCH (a:Person) RETURN a.* ) TO "person.parquet";
```

#### Copy To CSV Files
We added serveral configurations when exporting to CSV format.
```
COPY ( MATCH (a:Person) RETURN a.* ) TO "person.csv" (delim = '|', header=true);
```

We also improved the performance of CSV writer. Below is a micro benchmark of exporting LDBC100 Comment table to CSV format. 
```
COPY (MATCH (p:Comment) RETURN p.*) to ‘comment.csv’;
```

|             | Version 0.0.10   |  Version 0.1.0   |
|-------------|-----------|-----------|
| Runtime | 1239.3s    | 104.56s    |


#### Optional `column_names` Argument in Copy From Statements
Users can now load data to a subset of the columns in a table. Previously, we required that if
you are going to load an empty table T from a file F,
e.g., a CSV or Parquet file, then F contain: (1) as many columns as the columns in T; and (2) in the same order,
when table T was created. Now users can optionally add a `column_names` argument in `COPY FROM` statements,
which relaxes both of these restrictions: (1) F can now contain a subset of the columns; and (2) in arbitrary
order, which needs to be specified in the `column_names` argument. Here is an example:
```
CREATE NODE TABLE Person (id INT64, name STRING, comment STRING, PRIMARY KEY(id));
COPY Person (name, id) FROM "person.csv";
```
The code above first creates a `Person` table with 3 columns and then loads two of its columns from a file 
that contains `name` and `id` values of the columns respectively. 
The third `comment` column of the loaded records will be set to `NULL`. The details
of this feature can be found [here](../docs/cypher/copy.md).

### Updates

#### Detach Delete

Kùzu now supports `DETACH DELETE` which can delete a node and all its relationships.
The following query delete user "Adam" and all edges connected to "Adam".
```
MATCH (u:User) WHERE u.name = 'Adam' DETACH DELETE u;
```

To delete all records in the database, run
```
MATCH (n) DETACH DELETE n;
```

Details can be found [here](../docs/cypher/data-manipulation-clauses/delete.md#detach-delete).

#### Return Deleted Row

You can now retuned deleted records.
```
DELETE (a:Person) RETURN a.*;
```

Details can be found [here](../docs/cypher/data-manipulation-clauses/read-after-update.md).

#### Read Follows Update

We enable read after update in this release. This is mostly useful to create dependent records.

The following query tries to create two followers for "Adam". If the creation of "Karissa-follows->Adam" fails, i.e. there is no "Karissa", the second creation won't be executed.
```
MATCH (a:User {name:'Adam'})
WITH a
MATCH (b:User {name:'Karissa'}) CREATE (a)<-[e1:Follows {since:2023}]-(b)
WITH a
MATCH (c:User {name:'Zhang'}) CREATE (a)<-[e2:Follows {since:2024}]-(c)
```
Details can be found [here](../docs/cypher/data-manipulation-clauses/read-after-update.md)

### Others

#### Cast

We have moved to SQL-style casting function `cast(input, target type)`. User can cast an input expression to any type as long as a cast is defined between input type and target type. For example, `INT[]` to `INT` is not defined.
```
RETURN cast("[1,2,3]", "INT[]");
RETURN cast(1.2, "STRING");
```
Details can be found [here](../docs/cypher/expressions/casting.md).

#### INT128

We added 16 bytes huge integer.

#### Recursive Relationship Node Filter 

Kùzu now supports filtering intermediate node over recursive relationship.
```
Match p = (a:User)-[:Follows*1..2 (r, n | WHERE n.age > 21)]->(b:User) 
RETURN p;
```
Details can be found [here](../docs/cypher/query-clauses/match.md#filter-variable-length-relationships).

#### Count Subquery

Count subquery checks the number of matches for given pattern in the graph.

The following query counts number of followers for each user.
```
MATCH (a:User) 
RETURN a.name, COUNT { MATCH (a)<-[:Follows]-(b:User) } AS num_follower 
ORDER BY num_follower;
```
Details can be found [here](../docs/cypher/subquery.md#count-subquery).

## Development

### Nightly Build
We have setup a nightly build pipeline for those who want to access latest feature set. To use the latest nightly version of Kùzu, follow the instructions below:

For Python API, the latest nightly version can be installed with `pip install --pre kuzu`.

For Node.js API, the latest nightly version can be installed with `npm i kuzu@next`.

For Rust API, the latest nightly version can be found at [crates.io](https://crates.io/crates/kuzu/versions).

For CLI, C and C++ shared library, and Java JAR, the latest nightly version can be downloaded from the latest run of [this GitHub Actions pipeline](https://github.com/kuzudb/kuzu/actions/workflows/build-and-deploy.yml).

### Reduce Binary Size
We removed arrow as a third party dependency and obtain a signifcant drop of binary size. Additionally, we now strip the shared library and CLI binary we export. Stripping of our other libraries like Python and NodeJS is a work in progress. Together, we achieved the following improvement:

|             | Version 0.0.10   |  Version 0.1.0   |
|-------------|-----------|-----------|
| Binary Size[^2] | 27.2 MB   | 10.3 MB   |

[^1]: Each table consists of a SERIAL primary key column and a STRING column.
[^2]: Binary size is reported on MacOS arm64 platform.
